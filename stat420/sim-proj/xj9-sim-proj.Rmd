---
title: "Simulation Project"
author: "STAT 420, Summer 2017, Xiaoming Ji"
date: ''
output:
  html_document: 
    toc: yes
  pdf_document: default
---

## Simulation Study 1, Estimate Distributions

### Introduction
In this simulation study we will investigate the distribution of regression estimates $\hat{\beta}_1$, $s_e^2$ and $\hat{\text{E}}[Y]$ from MLR model

$$
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3}  + \beta_4 x_{i4} + \epsilon_i
$$
We will discuss how these estimates relate to true distributions and the impact of different $\sigma$.


### Methods

#### Setup
Assuming we have the known parameters for this MLR model,

- $\beta_0 = 2$
- $\beta_1 = 1$
- $\beta_2 = 1$
- $\beta_3 = 1$
- $\beta_4 = 1$
- $\epsilon_i \sim N(0, \sigma^2)$
- $\sigma \in (1, 5, 10)$

To investigate how the estimated parameters relate to these known parameters, we will simulate the training data from the sample data found in [`study_1.csv`](study_1.csv) (n = 15). We will predict $\hat{\text{E}}[Y]$ from the following $x_{eval}$:

- $x_1 = -3, x_2 = 2.5, x_3 = 0.5, x_4 = 0$, 

To start, we initialize the seed, set constants, load data and allocate memory for these estimates

```{r, message=FALSE}
library(readr)

birthday = 19720816
set.seed(birthday)

beta_0 = 2
beta_1 = 1
beta_2 = 1
beta_3 = 1
beta_4 = 1
sigma  = c(1, 5, 10)
x_eval = c(1, -3, 2.5, 0.5, 0)
p      = length(x_eval)

num_sims = 30

sim_data = read_csv("study_1.csv")
sample_size = nrow(sim_data)

beta_hat_1 = matrix(data = 0, nrow = num_sims, ncol = length(sigma))
se_square  = matrix(data = 0, nrow = num_sims, ncol = length(sigma))
y_hat_eval = matrix(data = 0, nrow = num_sims, ncol = length(sigma))
```

#### Model Analysis
As we know, given a known MLR model, we have $\hat{\beta}$ distribution as,

$$
\hat{\beta} \sim N\left(\beta, \sigma^2 \left(X^\top X\right)^{-1}  \right)
$$ 
We then have,
$$
\text{SD}[\hat{\beta}_1] = \sigma\sqrt{C_{22}}
$$
Where,
$$
C = \left(X^\top X\right)^{-1}
$$

For $\hat{y}(x)$, we have, 
$$
\hat{y}(x) \sim N \left(x^\top\beta, \sigma^2\left(x^\top\left(X^\top X\right)^{-1}x\right) \right)
$$
We then have,
$$
\text{E}[\hat{y}(x)] = x^\top\beta
$$
and,
$$
\text{SD}[\hat{y}(x)] = \sigma \sqrt{x^\top\left(X^\top X\right)^{-1}x}
$$
For $s_e$, we know it follow Chi-squared distribution with $(n-p)$ degrees of freedom. In our model, $n=15$, $p=5$, thus $df=10$, we have,
$$
\frac{10s_e^2}{\sigma^2} \sim \chi_{10}^2
$$

Let's compute $\text{SD}[{\beta_1}]$, $\text{E}[\hat{y}(x_{eval})]$ and $\text{SD}[\hat{y}(x_{eval})]$ for each $\sigma$ for later comparison.

```{r}
sd_beta_hat_1_true = rep(0, length(sigma))
sd_y_hat_eval_true = rep(0, length(sigma))

X = cbind(rep(1, sample_size), sim_data$x1, sim_data$x2, sim_data$x3, sim_data$x4)
C = solve(t(X) %*% X)

for (i in 1:length(sigma)) {
  sd_beta_hat_1_true[i] = sigma[i] * sqrt(C[1 + 1, 1 + 1])
  sd_y_hat_eval_true[i] = sigma[i] * sqrt(t(x_eval) %*% C %*% x_eval)
}

y_hat_eval_true = (t(x_eval) %*% c(beta_0, beta_1, beta_2, beta_3, beta_4))[1]
```


#### Simulation
We now perform the simulation `r num_sims` times for each $\sigma$. Each time, we update the $y$ variable in the data frame, leaving the $x$ variables the same. We then fit a model, and store the estimated $\hat{\beta}_1$, $s_e^2$ and $\hat{y}(x_{eval})$.

```{r}
for (s in 1:length(sigma)) {
  for (i in 1:num_sims) {
    eps           = rnorm(sample_size, mean = 0 , sd = sigma[s])
    sim_data$y    = beta_0 + beta_1 * sim_data$x1 + beta_2 * sim_data$x2 + 
                    beta_3 * sim_data$x3 + beta_4 * sim_data$x4 + eps
    fit           = lm(y ~ ., data = sim_data)
    
    beta_hat_1[i, s]  = coef(fit)[2]
    se_square[i, s]   = sum(fit$residuals ^ 2) / (sample_size - ncol(sim_data))
    y_hat_eval[i, s]  = predict(fit, newdata = data.frame(x1 = x_eval[2], x2 = x_eval[3],
                                                          x3 = x_eval[4], x4 = x_eval[5] ))
  }
}

```



### Results
We list the mean of $\hat{\beta}_1$ (Mean of Estimated Beta_1), mean of $s_e^2$ (Mean of Squred SE) and mean of $\hat{y}(x_{eval})$ (Mean of Estimated Y) as following table.

```{r}
beta_hat_1_mean = c(mean(beta_hat_1[,1]), mean(beta_hat_1[,2]), mean(beta_hat_1[,3]))
y_hat_eval_mean = c(mean(y_hat_eval[,1]), mean(y_hat_eval[,2]), mean(y_hat_eval[,3]))
se_square_mean  = c(mean(se_square[,1]), mean(se_square[,2]), mean(se_square[,3]))

table_data = data.frame(Sigma = sigma,
                        "Beta_1" = beta_1,
                        "Mean of Estimated Beta_1" = beta_hat_1_mean,
                        "Mean of Squred SE" = se_square_mean,
                        "True Y" = rep(y_hat_eval_true, length(sigma)),
                        "Mean of Estimated Y" = y_hat_eval_mean)
library("knitr")
kable(table_data, format = "markdown")
```

According to these results, we can conclude that 

- Mean of $\hat{\beta}_1$ is very close to $\beta_1$
- Mean of $s_e^2$ is very close to $\sigma^2$
- Mean of $\hat{y}(x_{eval})$ is very close to $y = 2 + 1 * (-3) +  1 * 2.5 + 1 * 0.5 + 1 * 0 = 2$

Let's plot the histograms of the simulated values, and overlay the true distribution with **yellow** curve.

```{r fig.width = 12, fig.height = 12}
par(mfrow = c(3,3))

for (i in 1:length(sigma)) {
  hist(beta_hat_1[,i], prob = TRUE, breaks = 20, 
        xlab = expression(hat(beta)[1]), main = paste("sigma=",sigma[i]), 
        border = "dodgerblue")
  curve(dnorm(x, mean = beta_1, sd = sd_beta_hat_1_true[i]), 
        col = "darkorange", add = TRUE, lwd = 3)
}

for (i in 1:length(sigma)) {
  hist(y_hat_eval[,i], prob = TRUE, breaks = 20, 
       xlab = expression(hat(y)), main = paste("sigma=",sigma[i]), border = "dodgerblue")
  curve(dnorm(x, mean = y_hat_eval_true, sd = sd_y_hat_eval_true[i]), 
        col = "darkorange", add = TRUE, lwd = 3)
}

se_qi = sqrt(se_square * (sample_size - p) / sigma[i] ^2)
  
for (i in 1:length(sigma)) {
  hist(se_qi[,i], prob = TRUE, breaks = 20, 
       xlab = expression(se^2), main = paste("sigma=",sigma[i]), border = "dodgerblue")
  curve(dchisq(x ^ 2, df = sample_size - p), 
        col = "darkorange", add = TRUE, lwd = 3)
}
```

### Discussion
```{r}
  hist(y_hat_eval[,i], prob = TRUE, breaks = 20, 
       xlab = expression(hat(y)), main = "sigma=1", border = "dodgerblue")
  curve(dnorm(x, mean = y_hat_eval_true, sd = sd_y_hat_eval_true[i]), 
        col = "darkorange", add = TRUE, lwd = 3)
  
  hist(se_square[,i], prob = TRUE, breaks = 20, 
       xlab = expression(se^2), main = "sigma=1", border = "dodgerblue")
```

<table style = "border-collapse: separate; border-spacing: 5px;">
  <tr>
    <td style = "width: 33%;">
      ```{r, echo = FALSE}

      ```
    </td>
    <td style = "width: 33%;">
      ```{r, echo = FALSE}

      ```
    </td>
    <td style = "width: 33%;">
      ```{r, echo = FALSE}

      ```
    </td>
  </tr>
  <tr>
    <td>
      Comments on graph in first column here
    </td>
    <td>
      Comments on graph in second column here
    </td>
    <td>
      Comments on graph in third column here
    </td>
  </tr>
  <tr>
    <td colspan = "3">
      This cell will span the entire row (aka across all three columns), and may be useful for making "section comments"
    </td>
  </tr>
</table>


## Simulation Study 2, RMSE for Selection?
### Introduction
### Methods
### Results
### Discussion


## Simulation Study 3, Power
### Introduction
### Methods
### Results
### Discussion

