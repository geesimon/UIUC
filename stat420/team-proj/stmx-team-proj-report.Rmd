---
title: "Proposal: Residential Housing Price Prediction"
author: "STAT 420, Summer 2017, Martynas Sapoka (netid:?), Shailender Singh (netid: ?), Tesa Ho (netid: tnho2), Xiaoming Ji (netid: xj9)"
netid: ""
date: ''
output:
  html_document: 
    toc: yes
  pdf_document: default
---


#Introduction
##Overview

We propose to create a linear model that can predict residential home prices in Ames, Iowa based on several explanatory variables.  This project is a practical example of using real world data that consists of a mix of different data types - nominal, ordinal, discrete, and continuous variables. 

Our final project will incorporate the following topics covered in this course: 

- Variable manipulation
- Outlier identification
- Data analysis and interpretation
- Model building
- Model evaluation

##Description of Dataset

The data set describes the sale of individual residential property in Ames, Iowa from 2006 to 2010 and contains 2,919 observations.  There are 80 explanatory variables (23 nominal, 23 ordinal, 14 discrete, and 20 continuous) involved in assessing home values.

The 23 nominal variables identify various types of dwellings, materials, garages, and environmental conditions.

The 23 ordinal variables range from 2 to 28 representing Streets (gravel or paved) and Neighborhoods (areas within the Ames city limits).

The 14 discrete variables quantify the number of items for each house.  Items include the number of kitchens, bedrooms, bathrooms, garage spaces and their respective location if the house has more than one floor. 

The 20 continuous variables are related to area dimensions for each house.  Variables include total dwelling square footage, total lot size, living area, and room dimensions. 

Some important variables that may have strong correlation with sale price.

- `MSZoning`: Identifies the general zoning classification of the sale.
- `LotArea`:  Lot size in square feet.
- `OverallQual`: Overall material and finish quality.
- `TotalBsmtSF`: Total square feet of basement area.
- `YearBuilt`: Original construction date.
- `BedroomAbvGr`: Bedrooms above grade (does NOT include basement bedrooms).
- `BsmtFullBath`:  Full bathrooms above grade.
- `GarageCars`: Size of garage in car capacity.

##Dataset Source

Inspired by [`Kaggle Competition`](https://www.kaggle.com/c/house-prices-advanced-regression-techniques), the data set is provided by Dean De Cock from Truman State University.  The raw data comes directly from the Iowa State Assessorʻs Office. The initial Excel file contained 113 variables describing 3,970 property sales that had occurred in Ames, Iowa between 2006 and 2010. 
The variables were a mix of nominal, ordinal, continuous, and discrete variables used in calculation of assessed values and included physical property measurements in addition to computation variables used in the city’s assessment process. Variables that required specific housing or assessing knowledge or previous calculations were removed from the final dataset.  

# Methods
## Check the data
We load the data and store it to variable `house_data`. Let's take a look on the data
```{r, include=FALSE}
library(readr)
house_data = read_csv("HousePrices/train.csv")
head(house_data)
```

```{r, echo=FALSE, fig.height=8, fig.width=8}
par(mfrow=c(3,2))
plot(SalePrice ~ GrLivArea, data = house_data, pch  = 20, cex  = 2, col  = "dodgerblue")
hist(house_data$SalePrice, col = "dodgerblue",border = "darkorange")
hist(house_data$GrLivArea, col = "dodgerblue",border = "darkorange")
hist(house_data$LotArea, col = "dodgerblue",border = "darkorange")
boxplot(SalePrice ~ OverallQual, data = house_data, pch = 20, cex = 2, col = "darkorange", border = "dodgerblue")
boxplot(SalePrice ~ Neighborhood, data = house_data, pch = 20, cex = 2, col = "darkorange", border = "dodgerblue")
```

## Data Preprocessing
###Change Predictor Name
R doesn't allow variable name start with a number, thus we change some column names to comply with R variable definition.
```{r}
BadNames =  c(FirstFlrSF = "1stFlrSF", SecondFlrSF = "2ndFlrSF", ThreeSsnPorch = "3SsnPorch")

for (i in 1:length(BadNames)) {
  colnames(house_data)[which(colnames(house_data) == BadNames[i])] = names(BadNames[i])
}
```

We stores all categorical predictor names in variable `CategoricalPredictors` and numerical predictor names in variable `NumericPredictors`.
```{r, include=FALSE}
CategoricalPredictors = c("MSSubClass", "MSZoning", "Street", "Alley", "LotShape",
            "LandContour", "Utilities", "LotConfig", "LandSlope", 
            "Neighborhood", "Condition1", "Condition2", "BldgType", 
            "HouseStyle", "OverallQual", "OverallCond", "RoofStyle", 
            "RoofMatl", "Exterior1st", "Exterior2nd", "MasVnrType",
            "ExterQual", "ExterCond", "Foundation", "BsmtQual", 
            "BsmtCond", "BsmtExposure", "BsmtFinType1", "BsmtFinType2", 
            "Heating", "HeatingQC", "CentralAir", "Electrical", 
            "KitchenQual", "Functional", "FireplaceQu", 
            "GarageType","GarageFinish", "GarageQual", "GarageCond", 
            "PavedDrive", "PoolQC", "Fence", "MiscFeature", 
            "SaleType", "SaleCondition", "MoSold")

NumericPredictors = colnames(house_data)[!colnames(house_data) %in% c(CategoricalPredictors, "Id")]
```
```{r}
(CategoricalPredictors)
(NumericPredictors)
```

###Missing Data
- Remove predictor with too many NAs (> 20%). In our data, NA is not a invalid value for most categorical predictors, but too many NA makes this predictor not very useful.
- Replace numerical NA with mean
- Convert categorical predictor to factor and make NA an extra value

```{r}
rm_names = c()
for(name in colnames(house_data)) {
  nas = is.na(house_data[[name]])
  
  if(sum(nas) / nrow(house_data) > 0.2) {
    print(paste("(", name, ") Removed"))
    rm_names = c(rm_names, name)
    
  } else {
    if(name %in% CategoricalPredictors) { #Categorical predictor
      house_data[[name]] = factor(house_data[[name]], exclude = NULL)
    } else{
      if(sum(nas) > 0) {
        house_data[[name]][nas] = mean(house_data[[name]][!nas])
      }
    }
  }
}

house_data = house_data[, !colnames(house_data) %in% rm_names]

#Remove these predictors from categorical predictors
CategoricalPredictors = CategoricalPredictors[CategoricalPredictors %in% colnames(house_data)]
#Remove these predictors from numerical predictors
NumericPredictors = NumericPredictors[NumericPredictors %in% colnames(house_data)]
```

###Normalize Data
According to the plots in section "Check the data", we can see distribution of `SalePrice` is not normalized (why it should be?). The variance of SalePrice ~ GrLivArea is also not constant. Check the summary of these 2 variables.
```{r}
summary(house_data$SalePrice)
summary(house_data$GrLivArea)
```
The range of these 2 variables are pretty large. To extend this thinking, it is very likely some variables for space size (like: GrLivArea, LotArea) may have same characteristic. We can normalize and stablize the data through log transformation. 

As the rule, we find the numerical predictors that have large range (max/min > 10) and replace it with log value.

Note: It is not recommended to change the data directly and better to use model formula instead. We do this to help our later analysis and model selection.
```{r}
var_names = c()
for(name in NumericPredictors) {
  if(range(house_data[[name]])[1] > 0 && 
     range(house_data[[name]])[2] / range(house_data[[name]])[1] > 10) {
    var_names = c(var_names, name)
  }
}

(var_names)
```
```{r}
for(name in var_names) {
  house_data[paste("Log_", name, sep = "")] = log(house_data[[name]])
}

house_data = house_data[, !colnames(house_data) %in% var_names]
```

We then replot the data.

```{r, echo=FALSE, fig.height=8, fig.width=8}
par(mfrow=c(3,2))
plot(Log_SalePrice ~ Log_GrLivArea, data = house_data, pch  = 20, cex  = 2, col  = "dodgerblue")
hist(house_data$Log_SalePrice, col = "dodgerblue",border = "darkorange")
hist(house_data$Log_GrLivArea, col = "dodgerblue",border = "darkorange")
hist(house_data$Log_LotArea, col = "dodgerblue",border = "darkorange")
boxplot(Log_SalePrice ~ OverallQual, data = house_data, pch = 20, cex = 2, col = "darkorange", border = "dodgerblue")
boxplot(Log_SalePrice ~ Neighborhood, data = house_data, pch = 20, cex = 2, col = "darkorange", border = "dodgerblue")
```
Except the Neighborhood plot doesn't have much change, other plots look much better.

###Combine predictors
```{r}
#Combine FullBath and HalfBath
house_data$Bath = house_data$FullBath + house_data$HalfBath * 0.5
house_data = subset(house_data, select = -c(Id, FullBath, HalfBath))
```

```{r, message=FALSE, include=FALSE}
#Utility functions

library(lmtest)
get_bp_decision = function(model, alpha) {
  decide = unname(bptest(model)$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_sw_decision = function(model, alpha) {
  decide = unname(shapiro.test(resid(model))$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

get_adj_r2 = function(model) {
  summary(model)$adj.r.squared
}

plot_fitted_resid = function(model, pointcol = "dodgerblue", linecol = "darkorange") {
  plot(fitted(model), resid(model), 
       col = pointcol, pch = 20, cex = 1.5,
       xlab = "Fitted", ylab = "Residuals")
  abline(h = 0, col = linecol, lwd = 2)
}

plot_qq = function(model, pointcol = "dodgerblue", linecol = "darkorange") {
  qqnorm(resid(model), col = pointcol, pch = 20, cex = 1.5)
  qqline(resid(model), col = linecol, lwd = 2)
}

build_formula = function(response_name, predictor_names, interaction = 1) {
  str_formula = paste(response_name, " ~ (")
  
  add_plus = FALSE
  for (predictor_name in predictor_names) {
    if(add_plus) str_formula = paste(str_formula, "+ ")
    else add_plus = TRUE
    
    str_formula = paste(str_formula, predictor_name)
  }
  str_formula = paste(str_formula, ")")
  
  if(interaction > 1) str_formula = paste(str_formula, "^ ", interaction)
  
  as.formula(str_formula)
}

#Get predictor true name from dummy variable names
get_predictor_name = function(all_names, dummy_names) {
  names = c()
  for(name in all_names) {
    result = pmatch(name, dummy_names)
    if(!is.na(result) && result > 0) {
      names = c(names, name)
    }
  }
  names
}
```

##Predictor Selection
To do: 
- Plot the correlation

We select the significant predictors that have high correlation with SalePrice.
```{r}
#We convert all factor variables to numeric in order to call cor()
num_house_data = house_data
for(name in colnames(num_house_data)){
  if(is.factor(num_house_data[[name]])) 
    num_house_data[[name]] = as.numeric(num_house_data[[name]])
}

all_cor = cor(num_house_data)
(sig_cor = sort(abs(all_cor["Log_SalePrice", abs(all_cor["Log_SalePrice",]) > 0.5]), decreasing = TRUE)[-1])
```

We first build a model with predictors that has cor values > 0.5
```{r, warning=FALSE}
library(faraway)
sig_model = lm(build_formula("Log_SalePrice", names(sig_cor)), data = house_data)
summary(sig_model)$adj.r.squared
sort(vif(sig_model)[vif(sig_model) > 5], decreasing = TRUE)
```
This model has good adjusted r-squared value. However, we do see many predictors has high VIFs. Although, high VIFs won't cause big problem to prediction, too many correlated predictors dose impact our capability to search through other better models using interaction or polynomial. This could also cause problem to analyze high leverage and calculate LOOCV RMSE because not able to solve $\left(X^\top X\right)^{-1}$.

We see `OverallQual` gives us the most trouble although VIF of `OverallQual2` is `r vif(sig_model)["OverallQual2"][[1]]`. Let's first try to remove this predictor and see what will happen.
```{r}
predictors = names(sig_cor)[names(sig_cor) != "OverallQual"]
no_overallqual_model = lm(build_formula("Log_SalePrice", predictors), data = house_data)
summary(no_overallqual_model)$adj.r.squared
anova(no_overallqual_model, sig_model)[2, "Pr(>F)"]
```
However, after removing `OverallQual`, adjusted r-squared dropped `r summary(sig_model)$adj.r.squared - summary(no_overallqual_model)$adj.r.squared`, the significant test also reject the $OverallQual = 0$ hypothesis.  All these evidences mean `OverallQual` is useful for our predicction.

If we check the `Log_SalePrice ~ OverallQual` plot above, we can see each value of `OverallQual` and the mapped mean of `Log_SalePrice` is quite linear. This means, increasing 1 score of `OverallQual` has constant change on `Log_SalePrice`. Therefore, we can convert this categorical predictor to numerical predictor.

```{r}
predictors = c(predictors, "OverallQual")
house_data$OverallQual = as.numeric(house_data$OverallQual)
model_1 = lm(build_formula("Log_SalePrice", names(sig_cor)), data = house_data)
summary(model_1)$adj.r.squared
```

To handle the remaining predictors, we make a function so that we can remove the predictor one-by-one. The reason we do this one-by-one instead of all of high VIF predictors together is because removing of one high VIF predictor could lower the VIF of other predictors. We try to keep as many predictors as we can in order to improve prediction.

```{r, warning=FALSE}
rm_highvif_predictor = function(y_name, x_names, data, nstep = 0) {
  repeat {
    m = lm(build_formula(y_name, x_names), data = data)
    high_vifs = vif(model_1)[vif(model_1) > 5]
    if(length(high_vifs) == 0) break

    print(sort(high_vifs, decreasing = TRUE)[1])
    rm_name = get_predictor_name(colnames(data), 
                                 names(sort(high_vifs, decreasing = TRUE))[1])
    print(rm_name)
    x_names = x_names[x_names != rm_name]
    
    nstep = nstep - 1
    if(nstep == 0) break
  }
  x_names
}

rm_highvif_predictor("Log_SalePrice", predictors, data = house_data, nstep = 2 )

```


Let's do the similar check on `GarageYrBlt` 
```{r, warning=FALSE}


predictors = predictors[predictors != "GarageYrBlt"]
model_2 = lm(build_formula("Log_SalePrice", predictors), data = house_data)
vif(model_2)[vif(model_2) > 5]
```
We can see removing of `GarageYrBlt`, the adjusted r-squared wasn't changed much. 

```{r}
predictors = predictors[predictors != "GarageType"]
model_3 = lm(build_formula("Log_SalePrice", predictors), data = house_data)
vif(model_3)[vif(model_3) > 5]
```

For the remaning variables, instead of checking them one by one, we do a exhausted search.
```{r}
library(leaps)
all_mod = summary(regsubsets(build_formula("Log_SalePrice", predictors), data = house_data, nvmax = 20))
all_mod$adjr2
```
```{r}


(predictors = names(all_mod$which[13,])[all_mod$which[13,]][-1])
predictors  = get_predictor_name(colnames(house_data), predictors)
  
model_4 = lm(build_formula("Log_SalePrice", predictors), data = house_data)
vif(model_4)[vif(model_4) > 5]
```

We remove all predictors that have VIF > 5.
```{r}
predictors = predictors[!predictors %in% c("ExterQual", "BsmtQual")]
model_5 = lm(build_formula("Log_SalePrice", predictors), data = house_data)
vif(model_5)[vif(model_5) > 5]
```
This give us a model that has no collinearity problem and also very easy to interpret.
```{r}
good_vif_model = model_5
coef(good_vif_model)
summary(good_vif_model)$r.squared
```
```

As side note, we could also remove the high VIF variable one by one. This will give us a model as,

```{r}
start_model = model_1

do {
  top_vif = sort(vif(model_1)[vif(model_1) > 5], decreasing = TRUE)[1]
  rm_predictor = get_predictor_name(colnames(house_data), c(names(top_vif)))
}
```



The VIFs of `GarageCars`, `TotalBsmtSF`, `KitchenQual`, `BsmtQual` and `GrLivArea` aren't that big. We perform search to find the best model.
```{r}
#model_aic = step(model_2, direction="backward", trace=0)
#model_bic = step(model_2, direction="backward", k = log(nrow(house_data)), trace=0)
#(rmse_aic  = get_loocv_rmse(model_aic))
#(rmse_bic  = get_loocv_rmse(model_bic))
```
According to LOOCV RMSE, BIC search gives us the best model to start. This model is very good for interpretation as it balances between number of predictors and RMSE. Since our goal it to make best predition, we should explore furthur to find the model with best RMSE. We assign this model to variable `start_model`.

```{r, include=FALSE}
#if(rmse_aic < rmse_aic) {
#  start_model = model_aic
#} else {
#  start_model = model_bic
#}
```
```{r}
#start_model = lm(formula = log(SalePrice) ~ OverallQual + log(GrLivArea) + GarageCars + BsmtQual + log(FirstFlrSF) + KitchenQual + GarageFinish + YearBuilt + YearRemodAdd, data = house_data)
#coef(start_model)
```

## Optimize for LOOCV RMSE 
We should give categorical predictors a second chance.

```{r}
#start_predictors = attr(summary(start_model)$term,"term.labels")
#remaining_predictors = colnames(house_data)[!colnames(house_data) %in% c(names(sig_cor), "SalePrice")]

#model = step(start_model, scope = build_formula("log(SalePrice)", all_predictors), direction = "forward", trace=0)
```

#Results 


#Discussion
- Whether we should make discrete variables in regression model a categorical predictors?

The way to discern an interval/ratio variable is to ask if every unit increment in the variable indicates the same amount of increment in the context you wish you measure. For instance, the jump from 35 to 36 degrees is the same as the jump from 43 to 44; it's the same amount of temperature difference. Likewise, the jump from 100 to 101 subscribers is the same as the jump from 1009 to 1010 subscribers. As long as this is true, your regression coefficient of that independent variable will make sense, because you can legitimately interpret it as the slope of the regression line.

General confusion appears when you mix in ordinal data, such as those 5-point "how satisfied are you?" questions. They are expressed in whole number, very easily to be confused with discrete data. However, each jump in the scale does not necessarily mean the same thing. E.g. a jump from "4: happy" to "5: very happy" is not necessarily the same as a jump from "1: very unhappy" to "2: unhappy." In that case, the variable should not be put into the regression as is, but treated differently (search "dummy variable in regression" to learn more.)

- Model Assumption 

- Kaggle Score


#Appendix
- [`Kaggle Competition`](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)
- Book
    - Applied Statistics with R by David Dalpiaz
    - An Introduction to Statistical Learning with Applications in R by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani
- [`Examining your data`](http://www.personal.psu.edu/jxb14/M554/articles/Hair%20et%20al%202010%20--%20Chapter%202.pdf)
- Data Description
